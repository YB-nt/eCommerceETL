[2023-09-15T15:47:52.452+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: eCommerce_ETL_dag.data_processing manual__2023-09-15T15:46:58.631124+00:00 [queued]>
[2023-09-15T15:47:52.466+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: eCommerce_ETL_dag.data_processing manual__2023-09-15T15:46:58.631124+00:00 [queued]>
[2023-09-15T15:47:52.467+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 2
[2023-09-15T15:47:52.485+0000] {taskinstance.py:1327} INFO - Executing <Task(BashOperator): data_processing> on 2023-09-15 15:46:58.631124+00:00
[2023-09-15T15:47:52.496+0000] {standard_task_runner.py:57} INFO - Started process 790 to run task
[2023-09-15T15:47:52.501+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'eCommerce_ETL_dag', 'data_processing', 'manual__2023-09-15T15:46:58.631124+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/workflow.py', '--cfg-path', '/tmp/tmp68kp8v_f']
[2023-09-15T15:47:52.506+0000] {standard_task_runner.py:85} INFO - Job 17: Subtask data_processing
[2023-09-15T15:47:52.576+0000] {task_command.py:410} INFO - Running <TaskInstance: eCommerce_ETL_dag.data_processing manual__2023-09-15T15:46:58.631124+00:00 [running]> on host 228226a2581e
[2023-09-15T15:47:52.691+0000] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='eCommerce_ETL_dag' AIRFLOW_CTX_TASK_ID='data_processing' AIRFLOW_CTX_EXECUTION_DATE='2023-09-15T15:46:58.631124+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-09-15T15:46:58.631124+00:00'
[2023-09-15T15:47:52.698+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2023-09-15T15:47:52.700+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'cd /usr/local && spark-submit --master spark://spark-master:7077 ./DataProcessing-assembly-0.1.jar']
[2023-09-15T15:47:52.715+0000] {subprocess.py:86} INFO - Output:
[2023-09-15T15:47:52.744+0000] {subprocess.py:93} INFO - /usr/local/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-09-15T15:47:55.795+0000] {subprocess.py:93} INFO - WARNING: An illegal reflective access operation has occurred
[2023-09-15T15:47:55.796+0000] {subprocess.py:93} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/assembly/target/scala-2.12/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2023-09-15T15:47:55.798+0000] {subprocess.py:93} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2023-09-15T15:47:55.800+0000] {subprocess.py:93} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2023-09-15T15:47:55.801+0000] {subprocess.py:93} INFO - WARNING: All illegal access operations will be denied in a future release
[2023-09-15T15:47:56.545+0000] {subprocess.py:93} INFO - 23/09/15 15:47:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-09-15T15:47:57.203+0000] {subprocess.py:93} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2023-09-15T15:51:08.549+0000] {subprocess.py:93} INFO - 23/09/15 15:51:08 ERROR Executor: Exception in task 1.0 in stage 19.0 (TID 709)
[2023-09-15T15:51:08.584+0000] {subprocess.py:93} INFO - java.io.EOFException: Cannot seek after EOF
[2023-09-15T15:51:08.585+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:354)
[2023-09-15T15:51:08.587+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:121)
[2023-09-15T15:51:08.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)
[2023-09-15T15:51:08.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:97)
[2023-09-15T15:51:08.596+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$3(CSVFileFormat.scala:138)
[2023-09-15T15:51:08.601+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
[2023-09-15T15:51:08.602+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:133)
[2023-09-15T15:51:08.603+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
[2023-09-15T15:51:08.605+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)
[2023-09-15T15:51:08.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
[2023-09-15T15:51:08.614+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:08.623+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:08.627+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:08.634+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2023-09-15T15:51:08.638+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2023-09-15T15:51:08.645+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2023-09-15T15:51:08.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:664)
[2023-09-15T15:51:08.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)
[2023-09-15T15:51:08.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)
[2023-09-15T15:51:08.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
[2023-09-15T15:51:08.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
[2023-09-15T15:51:08.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)
[2023-09-15T15:51:08.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-09-15T15:51:08.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:131)
[2023-09-15T15:51:08.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
[2023-09-15T15:51:08.686+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
[2023-09-15T15:51:08.696+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
[2023-09-15T15:51:08.709+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-09-15T15:51:08.712+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-09-15T15:51:08.714+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-09-15T15:51:08.717+0000] {subprocess.py:93} INFO - 23/09/15 15:51:08 ERROR TaskSetManager: Task 1 in stage 19.0 failed 1 times; aborting job
[2023-09-15T15:51:08.719+0000] {subprocess.py:93} INFO - Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 19.0 failed 1 times, most recent failure: Lost task 1.0 in stage 19.0 (TID 709) (228226a2581e executor driver): java.io.EOFException: Cannot seek after EOF
[2023-09-15T15:51:08.726+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:354)
[2023-09-15T15:51:08.729+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:121)
[2023-09-15T15:51:08.734+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)
[2023-09-15T15:51:08.737+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:97)
[2023-09-15T15:51:08.745+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$3(CSVFileFormat.scala:138)
[2023-09-15T15:51:08.749+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
[2023-09-15T15:51:08.757+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:133)
[2023-09-15T15:51:08.761+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
[2023-09-15T15:51:08.764+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)
[2023-09-15T15:51:08.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
[2023-09-15T15:51:08.769+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:08.778+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:08.781+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:08.783+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2023-09-15T15:51:08.785+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2023-09-15T15:51:08.789+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2023-09-15T15:51:08.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:664)
[2023-09-15T15:51:08.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)
[2023-09-15T15:51:08.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)
[2023-09-15T15:51:08.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
[2023-09-15T15:51:08.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
[2023-09-15T15:51:08.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)
[2023-09-15T15:51:08.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-09-15T15:51:08.809+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:131)
[2023-09-15T15:51:08.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
[2023-09-15T15:51:08.818+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
[2023-09-15T15:51:08.820+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
[2023-09-15T15:51:08.823+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-09-15T15:51:08.827+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-09-15T15:51:08.828+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-09-15T15:51:08.830+0000] {subprocess.py:93} INFO - 
[2023-09-15T15:51:08.831+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2023-09-15T15:51:08.832+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)
[2023-09-15T15:51:08.835+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)
[2023-09-15T15:51:08.836+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)
[2023-09-15T15:51:08.838+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-09-15T15:51:08.842+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-09-15T15:51:08.844+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-09-15T15:51:08.846+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)
[2023-09-15T15:51:08.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)
[2023-09-15T15:51:08.851+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)
[2023-09-15T15:51:08.854+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2023-09-15T15:51:08.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)
[2023-09-15T15:51:08.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)
[2023-09-15T15:51:08.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)
[2023-09-15T15:51:08.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)
[2023-09-15T15:51:08.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-09-15T15:51:08.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
[2023-09-15T15:51:08.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)
[2023-09-15T15:51:08.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)
[2023-09-15T15:51:08.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)
[2023-09-15T15:51:08.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)
[2023-09-15T15:51:08.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)
[2023-09-15T15:51:08.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-09-15T15:51:08.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-09-15T15:51:08.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
[2023-09-15T15:51:08.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)
[2023-09-15T15:51:08.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:893)
[2023-09-15T15:51:08.896+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
[2023-09-15T15:51:08.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-09-15T15:51:08.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-09-15T15:51:08.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-09-15T15:51:08.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-09-15T15:51:08.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
[2023-09-15T15:51:08.914+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-09-15T15:51:08.915+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-09-15T15:51:08.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-09-15T15:51:08.919+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-09-15T15:51:08.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-09-15T15:51:08.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
[2023-09-15T15:51:08.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
[2023-09-15T15:51:08.931+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
[2023-09-15T15:51:08.934+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
[2023-09-15T15:51:08.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
[2023-09-15T15:51:08.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-09-15T15:51:08.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-09-15T15:51:08.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-09-15T15:51:08.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-09-15T15:51:08.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-09-15T15:51:08.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
[2023-09-15T15:51:08.964+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
[2023-09-15T15:51:08.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
[2023-09-15T15:51:08.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
[2023-09-15T15:51:08.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
[2023-09-15T15:51:08.982+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
[2023-09-15T15:51:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
[2023-09-15T15:51:08.987+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
[2023-09-15T15:51:08.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-09-15T15:51:08.993+0000] {subprocess.py:93} INFO - 	at com.github.ybnt.DataProcessor$.loadData(DataProcessor.scala:101)
[2023-09-15T15:51:08.997+0000] {subprocess.py:93} INFO - 	at com.github.ybnt.DataProcessor$.main(DataProcessor.scala:203)
[2023-09-15T15:51:08.999+0000] {subprocess.py:93} INFO - 	at com.github.ybnt.DataProcessor.main(DataProcessor.scala)
[2023-09-15T15:51:09.001+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-09-15T15:51:09.003+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-09-15T15:51:09.005+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-09-15T15:51:09.011+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-09-15T15:51:09.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
[2023-09-15T15:51:09.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:966)
[2023-09-15T15:51:09.033+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:191)
[2023-09-15T15:51:09.037+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:214)
[2023-09-15T15:51:09.044+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
[2023-09-15T15:51:09.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1054)
[2023-09-15T15:51:09.048+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1063)
[2023-09-15T15:51:09.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[2023-09-15T15:51:09.059+0000] {subprocess.py:93} INFO - Caused by: java.io.EOFException: Cannot seek after EOF
[2023-09-15T15:51:09.062+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream.seek(ChecksumFileSystem.java:354)
[2023-09-15T15:51:09.064+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:121)
[2023-09-15T15:51:09.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)
[2023-09-15T15:51:09.076+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:97)
[2023-09-15T15:51:09.081+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$3(CSVFileFormat.scala:138)
[2023-09-15T15:51:09.083+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)
[2023-09-15T15:51:09.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:133)
[2023-09-15T15:51:09.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
[2023-09-15T15:51:09.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:187)
[2023-09-15T15:51:09.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
[2023-09-15T15:51:09.097+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:09.099+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:09.100+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-09-15T15:51:09.102+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.isEmpty(Iterator.scala:387)
[2023-09-15T15:51:09.103+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.isEmpty$(Iterator.scala:387)
[2023-09-15T15:51:09.107+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)
[2023-09-15T15:51:09.110+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:664)
[2023-09-15T15:51:09.113+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:895)
[2023-09-15T15:51:09.115+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:893)
[2023-09-15T15:51:09.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
[2023-09-15T15:51:09.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
[2023-09-15T15:51:09.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)
[2023-09-15T15:51:09.127+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-09-15T15:51:09.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:131)
[2023-09-15T15:51:09.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
[2023-09-15T15:51:09.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
[2023-09-15T15:51:09.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
[2023-09-15T15:51:09.135+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-09-15T15:51:09.137+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-09-15T15:51:09.147+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-09-15T15:51:10.717+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2023-09-15T15:51:10.838+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-09-15T15:51:10.884+0000] {taskinstance.py:1345} INFO - Marking task as UP_FOR_RETRY. dag_id=eCommerce_ETL_dag, task_id=data_processing, execution_date=20230915T154658, start_date=20230915T154752, end_date=20230915T155110
[2023-09-15T15:51:10.979+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 17 for task data_processing (Bash command failed. The command returned a non-zero exit code 1.; 790)
[2023-09-15T15:51:11.048+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-09-15T15:51:11.270+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
